name: Databricks CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
  DATABRICKS_CONFIG_FILE: "$HOME/.databrickscfg"

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pyspark delta-spark pyyaml python-dotenv databricks-cli
        pip install -e .
        
        # Configure Python path for tests
        echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV
        
    - name: Run tests
      run: |
        # Add root directory to PYTHONPATH
        export PYTHONPATH=$PYTHONPATH:${{ github.workspace }}
        # Explicitly specify test files to run with lower coverage threshold
        python -m pytest tests/test_api_client.py tests/test_config.py tests/test_db_utils.py tests/test_pipeline.py -v --cov=. --cov-fail-under=30
    
    - name: Configure Databricks CLI
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      env:
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      run: |
        # Verify environment variables are set
        echo "=== Verifying Environment Variables ==="
        echo "DATABRICKS_HOST is set: ${#DATABRICKS_HOST} characters"
        echo "DATABRICKS_TOKEN is set: ${#DATABRICKS_TOKEN} characters"
        
        # Verify databricks-cli is installed
        echo "=== Databricks CLI Version ==="
        databricks --version || echo "Failed to get databricks-cli version"
        
        # Test databricks-cli connectivity
        echo "=== Testing Databricks Connectivity ==="
        databricks clusters list || echo "Failed to list clusters"
        echo "===================================="
        
    - name: Deploy Notebooks to Databricks
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Define the target workspace path
        WORKSPACE_BASE="/Users/lnunesvalle@gmail.com/DataFusionX"
        
        # Create the base directory
        echo "Creating base directory: $WORKSPACE_BASE"
        databricks workspace mkdirs "$WORKSPACE_BASE"
        
        # Function to process a single notebook
        process_notebook() {
          local src_file="$1"
          local rel_path="${src_file#notebooks/}"
          local target_dir="$WORKSPACE_BASE/$(dirname "$rel_path")"
          
          # Create target directory if it doesn't exist
          if [[ "$target_dir" != "$WORKSPACE_BASE" ]]; then
            echo "Creating directory: $target_dir"
            databricks workspace mkdirs "$target_dir"
          fi
          
          # Import the notebook
          echo "Deploying $src_file to $target_dir"
          databricks workspace import \
            --language PYTHON \
            --overwrite \
            "$src_file" \
            "$WORKSPACE_BASE/$rel_path"
        }
        
        # Export the function so it's available in find -exec
        export -f process_notebook
        export WORKSPACE_BASE
        
        # Process all Python files in the notebooks directory
        echo "Deploying Python notebooks..."
        find notebooks -name "*.py" -type f -exec bash -c 'process_notebook "$0"' {} \;
        
        echo "âœ… Notebook deployment completed"
        
        # List the deployed notebooks for verification
        echo "\nðŸ“‹ Deployed notebooks in $WORKSPACE_BASE:"
        databricks workspace ls "$WORKSPACE_BASE"
    
    - name: Run Databricks Job
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Define the path to the main notebook
        MAIN_NOTEBOOK_PATH="/Users/lnunesvalle@gmail.com/DataFusionX/notebooks/bronze/ingest_coingecko.py"
        
        echo "ðŸš€ Starting Databricks job for notebook: $MAIN_NOTEBOOK_PATH"
        
        # Create a temporary job configuration
        JOB_CONFIG=$(cat <<EOF
        {
          "name": "Crypto Data Pipeline - $(date +'%Y%m%d-%H%M%S')",
          "existing_cluster_id": "${{ secrets.DATABRICKS_CLUSTER_ID }}",
          "libraries": [
            {
              "pypi": {
                "package": "requests"
              }
            }
          ],
          "notebook_task": {
            "notebook_path": "$MAIN_NOTEBOOK_PATH",
            "source": "WORKSPACE"
          },
          "max_retries": 1,
          "timeout_seconds": 3600
        }
        EOF
        )
        
        # Create and run the job
        echo "Creating and running Databricks job..."
        JOB_ID=$(databricks jobs create --json "$JOB_CONFIG" | jq -r '.job_id')
        
        if [ -z "$JOB_ID" ] || [ "$JOB_ID" = "null" ]; then
          echo "âŒ Failed to create job"
          exit 1
        fi
        
        echo "âœ… Job created with ID: $JOB_ID"
        echo "ðŸš€ Starting job run..."
        
        # Run the job
        RUN_ID=$(databricks jobs run-now --job-id "$JOB_ID" | jq -r '.run_id')
        
        if [ -z "$RUN_ID" ] || [ "$RUN_ID" = "null" ]; then
          echo "âŒ Failed to start job run"
          exit 1
        fi
        
        echo "âœ… Job run started with ID: $RUN_ID"
        echo "ðŸ”— View the run in Databricks: ${{ secrets.DATABRICKS_HOST }}#job/$JOB_ID/run/$RUN_ID"
