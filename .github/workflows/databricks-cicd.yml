name: Databricks CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
  DATABRICKS_CONFIG_FILE: "$HOME/.databrickscfg"

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pyspark delta-spark pyyaml python-dotenv databricks-cli
        pip install -e .
        
        # Configure Python path for tests
        echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV
        
    - name: Run tests
      run: |
        # Add root directory to PYTHONPATH
        export PYTHONPATH=$PYTHONPATH:${{ github.workspace }}
        # Explicitly specify test files to run with lower coverage threshold
        python -m pytest tests/test_api_client.py tests/test_config.py tests/test_db_utils.py tests/test_pipeline.py -v --cov=. --cov-fail-under=30
    
    - name: Configure Databricks CLI
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      env:
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      run: |
        # Verify environment variables are set
        echo "=== Verifying Environment Variables ==="
        echo "DATABRICKS_HOST is set: ${#DATABRICKS_HOST} characters"
        echo "DATABRICKS_TOKEN is set: ${#DATABRICKS_TOKEN} characters"
        
        # Verify databricks-cli is installed
        echo "=== Databricks CLI Version ==="
        databricks --version || echo "Failed to get databricks-cli version"
        
        # Test databricks-cli connectivity
        echo "=== Testing Databricks Connectivity ==="
        databricks clusters list || echo "Failed to list clusters"
        echo "===================================="
        
    - name: Deploy Notebooks to Databricks
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Define the target workspace path
        WORKSPACE_PATH="/Users/lnunesvalle@gmail.com/DataFusionX/notebooks"
        
        # Create target directory in user's workspace
        echo "Creating directory: $WORKSPACE_PATH"
        databricks workspace mkdirs "$WORKSPACE_PATH"
        
        # Deploy all notebooks from the notebooks directory
        echo "Deploying notebooks to Databricks workspace..."
        for notebook in notebooks/*.py; do
          if [ -f "$notebook" ]; then
            notebook_name=$(basename "$notebook")
            dest_path="$WORKSPACE_PATH/$notebook_name"
            echo "Deploying $notebook to $dest_path"
            databricks workspace import --language PYTHON --overwrite "$notebook" "$dest_path"
          fi
        done
        
        echo "âœ… Notebooks deployed successfully to $WORKSPACE_PATH"
    
    - name: Run Databricks Job
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Get the job ID from the workflow file
        JOB_ID=$(jq -r '.job_id' workflows/pipeline_job.json)
        
        if [ -z "$JOB_ID" ] || [ "$JOB_ID" = "null" ]; then
          echo "No job ID found in pipeline_job.json. Trying to get job by name..."
          JOB_ID=$(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name == "crypto_data_pipeline") | .job_id' | head -n 1)
        fi
        
        if [ -z "$JOB_ID" ]; then
          echo "Error: Could not find job ID. Please check your pipeline_job.json file or create the job manually."
          exit 1
        fi
        
        echo "Running Databricks job with ID: $JOB_ID"
        databricks jobs run-now --job-id $JOB_ID
        
        echo "Job started successfully!"
