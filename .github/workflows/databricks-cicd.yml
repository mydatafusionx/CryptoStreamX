name: Databricks CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
  DATABRICKS_CONFIG_FILE: "$HOME/.databrickscfg"

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pyspark delta-spark pyyaml python-dotenv
        pip install -e .
        
        # Configure Python path for tests
        echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV
        
    - name: Install Databricks CLI with pip
      run: |
        # Install the latest pre-release version of Databricks CLI
        echo "Installing Databricks CLI with pip..."
        pip install --upgrade "databricks-cli>=0.18.0" --pre
        
        # Install SQL CLI extension
        pip install --upgrade databricks-sql-cli
        
        # Verify installation
        echo "=== Installed CLI versions ==="
        databricks -v || echo "Databricks CLI not found"
        which databricks || echo "Databricks CLI not in PATH"
        
        # Create a simple test script to verify SQL commands work
        echo '#!/bin/bash
        echo "Testing Databricks SQL CLI..."
        databricks sql --version || echo "SQL CLI not found"
        ' > test_sql_cli.sh
        chmod +x test_sql_cli.sh
        ./test_sql_cli.sh
        
    - name: Run basic tests
      run: |
        # Add root directory to PYTHONPATH
        export PYTHONPATH=$PYTHONPATH:${{ github.workspace }}
        # Run only critical tests without coverage requirements
        python -m pytest tests/test_config.py -v
      env:
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        DATABRICKS_WAREHOUSE_ID: ${{ secrets.DATABRICKS_WAREHOUSE_ID }}
      continue-on-error: true
        
    - name: Configure Databricks CLI
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      env:
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      run: |
        # Verify environment variables are set
        echo "=== Verifying Environment Variables ==="
        echo "DATABRICKS_HOST is set: ${#DATABRICKS_HOST} characters"
        echo "DATABRICKS_TOKEN is set: ${#DATABRICKS_TOKEN} characters"
        
        # Verify databricks-cli is installed
        echo "=== Databricks CLI Version ==="
        databricks --version || echo "Failed to get databricks-cli version"
        
        # Test databricks-cli connectivity
        echo "=== Testing Databricks Connectivity ==="
        databricks clusters list || echo "Failed to list clusters"
        echo "===================================="
        
    - name: Deploy Notebooks to Databricks
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Define the target workspace path
        WORKSPACE_BASE="/Users/lnunesvalle@gmail.com/DataFusionX"
        
        # Create the base directory
        echo "Creating base directory: $WORKSPACE_BASE"
        databricks workspace mkdirs "$WORKSPACE_BASE"
        
        # Function to process a single notebook
        process_notebook() {
          local src_file="$1"
          local rel_path="${src_file#notebooks/}"
          local target_dir="$WORKSPACE_BASE/$(dirname "$rel_path")"
          
          # Create target directory if it doesn't exist
          if [[ "$target_dir" != "$WORKSPACE_BASE" ]]; then
            echo "Creating directory: $target_dir"
            databricks workspace mkdirs "$target_dir"
          fi
          
          # Import the notebook
          echo "Deploying $src_file to $target_dir"
          databricks workspace import \
            --language PYTHON \
            --overwrite \
            "$src_file" \
            "$WORKSPACE_BASE/$rel_path"
        }
        
        # Export the function so it's available in find -exec
        export -f process_notebook
        export WORKSPACE_BASE
        
        # Process all Python files in the notebooks directory
        echo "Deploying Python notebooks..."
        find notebooks -name "*.py" -type f -exec bash -c 'process_notebook "$0"' {} \;
        
        echo "âœ… Notebook deployment completed"
        
        # List the deployed notebooks for verification
        echo "\nðŸ“‹ Deployed notebooks in $WORKSPACE_BASE:"
        databricks workspace ls "$WORKSPACE_BASE"
    
    - name: Run SQL Queries on Warehouse
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Define SQL Warehouse ID (from your Databricks SQL Warehouse)
        WAREHOUSE_ID="${{ secrets.DATABRICKS_WAREHOUSE_ID }}"
        
        echo "ðŸš€ Starting SQL execution on Warehouse ID: $WAREHOUSE_ID"
        
        # Create a temporary file for SQL queries
        cat > queries.sql << 'EOF'
        -- Create example table if not exists
        CREATE TABLE IF NOT EXISTS example_table (
          id INT,
          name STRING,
          value DOUBLE
        );
        
        -- Insert sample data
        INSERT INTO example_table VALUES
          (1, 'test', 3.14);
          
        -- Query the data
        SELECT * FROM example_table;
        EOF
        
        # Execute SQL using the Databricks CLI (legacy method)
        echo "ðŸ“ Executing SQL queries..."
        
        # Set Databricks config
        echo "Setting up Databricks configuration..."
        cat > $HOME/.databrickscfg << EOF
        [DEFAULT]
        host = ${{ secrets.DATABRICKS_HOST }}
        token = ${{ secrets.DATABRICKS_TOKEN }}
        EOF
        
        # Execute SQL using the Databricks SQL CLI (legacy)
        echo "Executing SQL file..."
        databricks workspace import \
          --language SQL \
          --overwrite \
          queries.sql \
          "/tmp/run_queries.sql"
          
        # Run the SQL job
        echo "Running SQL job..."
        JOB_ID=$(databricks jobs create \
          --json '{
            "name": "CI_CD_SQL_Job",
            "tasks": [
              {
                "task_key": "run_sql",
                "sql_task": {
                  "query": {
                    "query_id": "$(databricks sql queries create \
                      --data @- <<EOF
                      {
                        "data_source_id": "$WAREHOUSE_ID",
                        "query": "$(cat queries.sql | tr '\n' ' ' | sed 's/"/\\"/g')",
                        "name": "CI_CD_Query"
                      }
                      EOF
                    )"
                  },
                  "warehouse_id": "$WAREHOUSE_ID"
                }
              }
            ]
          }' | jq -r '.job_id')
          
        # Run the job and wait for completion
        echo "Job ID: $JOB_ID"
        RUN_ID=$(databricks jobs run-now --job-id $JOB_ID | jq -r '.run_id')
        
        # Wait for job completion
        echo "Waiting for job to complete..."
        while :; do
          STATUS=$(databricks runs get --run-id $RUN_ID | jq -r '.state.life_cycle_state')
          echo "Current status: $STATUS"
          [[ "$STATUS" == "TERMINATED" || "$STATUS" == "SKIPPED" || "$STATUS" == "INTERNAL_ERROR" ]] && break
          sleep 10
        done
        
        # Get final status
        FINAL_STATUS=$(databricks runs get --run-id $RUN_ID | jq -r '.state.result_state')
        
        if [ "$FINAL_STATUS" == "SUCCESS" ]; then
          echo "âœ… SQL execution completed successfully!"
        else
          echo "âŒ SQL execution failed with status: $FINAL_STATUS"
          databricks runs get-output --run-id $RUN_ID
          exit 1
        fi
        
        echo "ðŸ”— View your SQL Warehouse: ${{ secrets.DATABRICKS_HOST }}#sql/warehouses/$WAREHOUSE_ID"
